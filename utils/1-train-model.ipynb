{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmt lk 184 kuni 187.\n",
    "NÃ¤idiskood: https://github.com/PacktPublishing/TinyML-Cookbook/blob/main/Chapter05/ColabNotebooks/prepare_model.ipynb\n",
    "\n",
    "Tauno Erik 14.08.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import zipfile\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ALPHA = 0.35\n",
    "MODEL_INPUT_WIDTH = 48\n",
    "MODEL_INPUT_HEIGHT = 48\n",
    "TFL_MODEL_FILE = \"scene_recognition.tflite\"\n",
    "TFL_MODEL_HEADER_FILE = \"scene_recognition_model.h\"\n",
    "TF_MODEL = \"scene_recognition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Onen in Colab and upload dataset.zip to root directory. (For me it does not work!)\n",
    "Or download dataset directly from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/taunoe/flower-camera/raw/main/dataset.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Unzip the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "  zip_ref.extractall(\".\")\n",
    "\n",
    "data_dir = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Resizes the input images to 48x48 with the bilinear interpolation. Prepare the training (80%) and validation (20%) datsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  interpolation=\"bilinear\",\n",
    "  image_size=(MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT)) # const\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  interpolation=\"bilinear\",\n",
    "  image_size=(MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get the name of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rescale the pixel values from [0, 255] to [-1, 1] because the pre-trained model expects this interval data range for the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = tf.keras.layers.Rescaling(1./255, offset= -1)\n",
    "train_ds = train_ds.map(lambda x, y: (rescale(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (rescale(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Import the MobileNet v2 pre-trained model with the weights trained on the ImageNet dataset and alpha=0.35. Furthermore, set the input image at the lowest resolution allowed by the pre-trained model (48, 48, 3) and exclude the top (fully-connected) layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(input_shape=(48, 48, 3),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet',\n",
    "                         alpha=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Freeze the weights so that you do not update these values during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "feat_extr = base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Augment the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmen = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (augmen(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (augmen(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Prepare the classification head with a global pooling followed by a dense layer with a softmax activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(num_classes,activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Build the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT, 3))\n",
    "x = global_avg_layer(feat_extr.layers[-1].output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = dense_layer(x)\n",
    "model = tf.keras.Model(inputs=feat_extr.inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Compile the model with a 0.0005 learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Train the model with 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.  Save the TensorFlow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(TF_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and testing the quantized TFLite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Unzip the test dataset (test_samples.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/taunoe/flower-camera/raw/main/test_samples.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"test_samples.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "    \n",
    "test_dir = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Rescale the pixel values from [0,255] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
    "                                                      interpolation=\"bilinear\",\n",
    "                                                      image_size=(MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT))\n",
    "test_ds  = test_ds.map(lambda x, y: (rescale(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Quantize the TensorFlow model with the TFLite converter to TensorFlow Lite format (FlatBuffers). Apply the 8-bit quantization to the entire model except for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_ds = test_ds.unbatch()\n",
    "\n",
    "def representative_data_gen():\n",
    "  for i_value, o_value in repr_ds.batch(1).take(48):\n",
    "    yield [i_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(TF_MODEL)\n",
    "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "\n",
    "tfl_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Get the TFLite model size in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_tfl_model = len(tfl_model)\n",
    "print(len(tfl_model), \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Initialize the TFLite interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tfl_model)\n",
    "\n",
    "# Allocate the tensors\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Get input quantization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input/output layer information\n",
    "i_details = interpreter.get_input_details()[0]\n",
    "o_details = interpreter.get_output_details()[0]\n",
    "\n",
    "# Get input quantization parameters.\n",
    "i_quant = i_details[\"quantization_parameters\"]\n",
    "i_scale      = i_quant['scales'][0]\n",
    "i_zero_point = i_quant['zero_points'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Evaluate the accuracy of the quantized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds0 = val_ds.unbatch()\n",
    "\n",
    "num_correct_samples = 0\n",
    "num_total_samples   = len(list(test_ds0.batch(1)))\n",
    "\n",
    "for i_value, o_value in test_ds0.batch(1):\n",
    "  i_value = (i_value / i_scale) + i_zero_point\n",
    "  i_value = tf.cast(i_value, dtype=tf.int8)\n",
    "  interpreter.set_tensor(i_details[\"index\"], i_value)\n",
    "  interpreter.invoke()\n",
    "  o_pred = interpreter.get_tensor(o_details[\"index\"])[0]\n",
    "\n",
    "  if np.argmax(o_pred) == o_value:\n",
    "    num_correct_samples += 1\n",
    "\n",
    "print(\"Accuracy:\", num_correct_samples/num_total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Convert the TFLite model to C-byte array with xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"model.tflite\", \"wb\").write(tfl_model)\n",
    "!apt-get update && apt-get -qq install xxd\n",
    "!xxd -c 60 -i model.tflite > scene_recognition.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait and download the scene_recognition.h file from Colab's left pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
